{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0384cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5207f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palak Jain\\AppData\\Local\\Temp\\ipykernel_31652\\3126214323.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"qwen2.5:7b\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Ollama model (llama3 is used here)\n",
    "# llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "llm = ChatOllama(model=\"qwen2.5:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bf45dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palak Jain\\AppData\\Local\\Temp\\ipykernel_31652\\1008887214.py:2: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([HumanMessage(content=\"What is the capital of France?\")])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLaMA: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Send a sample prompt\n",
    "response = llm([HumanMessage(content=\"What is the capital of France?\")])\n",
    "\n",
    "print(\"Response from LLaMA:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ef595b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to LLM. Please check your API key and network connection.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m llm=ChatOllama(model=\u001b[33m\"\u001b[39m\u001b[33mllama3.2\u001b[39m\u001b[33m\"\u001b[39m, num_ctx=\u001b[32m32000\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create agent with the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m agent = \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run()\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025_Projects\\browser_use\\browser_use\\Lib\\site-packages\\browser_use\\utils.py:311\u001b[39m, in \u001b[36mtime_execution_sync.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m    310\u001b[39m \tstart_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \tresult = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \texecution_time = time.time() - start_time\n\u001b[32m    313\u001b[39m \t\u001b[38;5;66;03m# Only log if execution takes more than 0.25 seconds\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025_Projects\\browser_use\\browser_use\\Lib\\site-packages\\browser_use\\agent\\service.py:214\u001b[39m, in \u001b[36mAgent.__init__\u001b[39m\u001b[34m(self, task, llm, page, browser, browser_context, browser_profile, browser_session, controller, sensitive_data, initial_actions, register_new_step_callback, register_done_callback, register_external_agent_status_raise_error_callback, use_vision, use_vision_for_planner, save_conversation_path, save_conversation_path_encoding, max_failures, retry_delay, override_system_message, extend_system_message, max_input_tokens, validate_output, message_context, generate_gif, available_file_paths, include_attributes, max_actions_per_step, tool_calling_method, page_extraction_llm, planner_llm, planner_interval, is_planner_reasoning, extend_planner_system_message, injected_agent_state, context, save_playwright_script_path, enable_memory, memory_config, source)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m._set_model_names()\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Verify we can connect to the LLM and setup the tool calling method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_verify_and_setup_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# Handle users trying to use use_vision=True with DeepSeek models\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdeepseek\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_name.lower():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025_Projects\\browser_use\\browser_use\\Lib\\site-packages\\browser_use\\agent\\service.py:1775\u001b[39m, in \u001b[36mAgent._verify_and_setup_llm\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_verify_and_setup_llm\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   1771\u001b[39m \u001b[38;5;250m\t\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[33;03m\tVerify that the LLM API keys are setup and the LLM API is responding properly.\u001b[39;00m\n\u001b[32m   1773\u001b[39m \u001b[33;03m\tAlso handles tool calling method detection if in auto mode.\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[33;03m\t\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m \t\u001b[38;5;28mself\u001b[39m.tool_calling_method = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_tool_calling_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1777\u001b[39m \t\u001b[38;5;66;03m# Skip verification if already done\u001b[39;00m\n\u001b[32m   1778\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, \u001b[33m'\u001b[39m\u001b[33m_verified_api_keys\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m SKIP_LLM_API_KEY_VERIFICATION:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025_Projects\\browser_use\\browser_use\\Lib\\site-packages\\browser_use\\agent\\service.py:754\u001b[39m, in \u001b[36mAgent._set_tool_calling_method\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    749\u001b[39m \tlogger.debug(\n\u001b[32m    750\u001b[39m \t\t\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mKnown method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mknown_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.chat_model_library\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, falling back to detection\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    751\u001b[39m \t)\n\u001b[32m    753\u001b[39m \u001b[38;5;66;03m# Auto-detect the best method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_detect_best_tool_calling_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025_Projects\\browser_use\\browser_use\\Lib\\site-packages\\browser_use\\agent\\service.py:646\u001b[39m, in \u001b[36mAgent._detect_best_tool_calling_method\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    643\u001b[39m \t\t\t\u001b[38;5;28;01mreturn\u001b[39;00m method\n\u001b[32m    645\u001b[39m \u001b[38;5;66;03m# If we get here, no methods worked\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mFailed to connect to LLM. Please check your API key and network connection.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to LLM. Please check your API key and network connection."
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from browser_use import Agent\n",
    "from pydantic import SecretStr\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "# llm=ChatOllama(model=\"llama3.2:1b\", num_ctx=32000)\n",
    "llm=ChatOllama(model=\"llama3.2\", num_ctx=32000)\n",
    "\n",
    "# Create agent with the model\n",
    "agent = Agent(\n",
    "    task=\"What is the capital of France?\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "result = await agent.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5375d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "browser_use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
